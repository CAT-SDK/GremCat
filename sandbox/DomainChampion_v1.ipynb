{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nVPskRiw15K"
   },
   "source": [
    "##**Domain Champion Data**\n",
    "By: Armando Acosta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY7vSBdbw15Z"
   },
   "source": [
    "Notebook Overview:\n",
    "This notebook aims to help display various data and information on the development patterns of various software repositories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMoMbglEw15d"
   },
   "source": [
    "***Table of Top Contributors***\n",
    "\n",
    "This first half of the script simply retrieves the GitHub usernames of the top contributors for each project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kq9Nt8vEzXob"
   },
   "source": [
    "**First:** Create dictionary object of project names and their url path. This helps to iterate through each project and ensure the API handles the links correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-y97eoxLzoaz"
   },
   "outputs": [],
   "source": [
    "project_dict = {\n",
    "    'Moose': 'idaholab/moose',\n",
    "    'Spack': 'spack/spack',\n",
    "    'yt': 'yt-project/yt',\n",
    "    'petsc': 'petsc/petsc',\n",
    "    'E3SM': 'E3SM-Project/E3SM',\n",
    "    'lammps': 'lammps/lammps',\n",
    "    'gromacs': 'gromacs/gromacs',\n",
    "    'OSGConnect': 'OSGConnect/TOREVIEW-tutorial-namd',\n",
    "    'QMCPACK': 'QMCPACK/qmcpack',\n",
    "    'Nek5000': 'Nek5000/Nek5000',\n",
    "    'nwchemgit': 'nwchemgit/nwchem',\n",
    "    # 'ECP-astro': '???', No url for this project\n",
    "    'lanl': 'lanl/LATTE',\n",
    "    'CRL': 'gridaphobe/CRL',\n",
    "    'enzo-project': 'enzo-project/enzo-dev'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSm-gTF1z_U0"
   },
   "source": [
    "**Second:** The main function for retrieving the information is created. Here, we set up to write the information to a separate file and create another dictionary to use the pandas library to assist in printing out the developer names in a DataFrame (we also use parameters passed in from main to use user-inputted login credentials to access the GitHub API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b73kGLpy1WiM"
   },
   "outputs": [],
   "source": [
    "def retrieve_repo_info(uname, pword):\n",
    "    devfile = open('dev_info.txt', 'w')\n",
    "    devfile.write('Repository Name/Top Contributor/Number of Contributions')\n",
    "    print(file=devfile)\n",
    "\n",
    "    data = {'Repository Name': [],\n",
    "            'Top Contributor': [],\n",
    "            'Number of Contributions': []}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    username = input('GitHub Authentication Username: ')\n",
    "    password = input('Password: ')\n",
    "    retrieve_repo_info(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_T4ypfPT5BXG"
   },
   "source": [
    "**Third:** The function uses a for loop to iterate through each repository and return a repository's respective, top contributor. This information is both placed in a file and printed to the terminal in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfnN2QC25YH3"
   },
   "outputs": [],
   "source": [
    "for project in project_dict:\n",
    "        # Following block for writing data to file\n",
    "        print(' ', file=devfile)\n",
    "        url = 'https://api.github.com/repos/%s/contributors' % project_dict[project]\n",
    "\n",
    "        response = requests.get(url, auth=(uname, pword))\n",
    "        json_response = response.json()\n",
    "\n",
    "        print(project, file=devfile)\n",
    "        print('Top Contributor:', json_response[0]['login'], file=devfile)\n",
    "        print('Contributions:', json_response[0]['contributions'], file=devfile)\n",
    "        print('-----------------------------------', file=devfile)\n",
    "\n",
    "        # Following block for creating dataframe\n",
    "        data['Repository Name'].append(project)\n",
    "        data['Top Contributor'].append(json_response[0]['login'])\n",
    "        data['Number of Contributions'].append(json_response[0]['contributions'])\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhRUQZaa6uZP"
   },
   "source": [
    "**Final Script:** The final script is put together below. When ran, it prints the results of the top contributors. Example provided below with results at time of running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "nNIW-VvMw15g",
    "outputId": "8bfa6dcc-dec2-46c0-97b7-67a53f40d94b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub Authentication Username: aacosta13\n",
      "Password: ··········\n",
      "   Repository Name Top Contributor  Number of Contributions\n",
      "0            Moose        permcody                     6251\n",
      "1            Spack        tgamblin                     2426\n",
      "2               yt     matthewturk                     8319\n",
      "3            petsc      BarrySmith                    20601\n",
      "4             E3SM      jedwards4b                     3747\n",
      "5           lammps        akohlmey                     5396\n",
      "6          gromacs        mabraham                     1721\n",
      "7       OSGConnect          dmbala                       31\n",
      "8          QMCPACK          ye-luo                     3991\n",
      "9          Nek5000          stgeke                      450\n",
      "10       nwchemgit         edoapra                     5439\n",
      "11            lanl          cnegre                      163\n",
      "12             CRL      gridaphobe                      132\n",
      "13    enzo-project     matthewturk                      558\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "'''\n",
    "Small script to retrieve domain champ info from different repos. Prints to file and creates a pandas dataframe\n",
    "\n",
    "By: Armando Acosta\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import getpass\n",
    "\n",
    "project_dict = {\n",
    "    'Moose': 'idaholab/moose',\n",
    "    'Spack': 'spack/spack',\n",
    "    'yt': 'yt-project/yt',\n",
    "    'petsc': 'petsc/petsc',\n",
    "    'E3SM': 'E3SM-Project/E3SM',\n",
    "    'lammps': 'lammps/lammps',\n",
    "    'gromacs': 'gromacs/gromacs',\n",
    "    'OSGConnect': 'OSGConnect/TOREVIEW-tutorial-namd',\n",
    "    'QMCPACK': 'QMCPACK/qmcpack',\n",
    "    'Nek5000': 'Nek5000/Nek5000',\n",
    "    'nwchemgit': 'nwchemgit/nwchem',\n",
    "    # 'ECP-astro': '???', no url for this project\n",
    "    'lanl': 'lanl/LATTE',\n",
    "    'CRL': 'gridaphobe/CRL',\n",
    "    'enzo-project': 'enzo-project/enzo-dev'\n",
    "}\n",
    "\n",
    "\n",
    "def retrieve_repo_info(uname, pword):\n",
    "    devfile = open('dev_info.txt', 'w')\n",
    "    devfile.write('Repository Name/Top Contributor/Number of Contributions')\n",
    "    print(file=devfile)\n",
    "\n",
    "    data = {'Repository Name': [],\n",
    "            'Top Contributor': [],\n",
    "            'Number of Contributions': []}\n",
    "\n",
    "    for project in project_dict:\n",
    "        # Following block for writing data to file\n",
    "        print(' ', file=devfile)\n",
    "        url = 'https://api.github.com/repos/%s/contributors' % project_dict[project]\n",
    "\n",
    "        response = requests.get(url, auth=(uname, pword))\n",
    "        json_response = response.json()\n",
    "\n",
    "        print(project, file=devfile)\n",
    "        print('Top Contributor:', json_response[0]['login'], file=devfile)\n",
    "        print('Contributions:', json_response[0]['contributions'], file=devfile)\n",
    "        print('-----------------------------------', file=devfile)\n",
    "\n",
    "        # Following block for creating dataframe\n",
    "        data['Repository Name'].append(project)\n",
    "        data['Top Contributor'].append(json_response[0]['login'])\n",
    "        data['Number of Contributions'].append(json_response[0]['contributions'])\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    username = input('GitHub Authentication Username: ')\n",
    "    password = getpass.getpass(prompt='Password: ')\n",
    "    retrieve_repo_info(username, password)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2dlE2sodsZP"
   },
   "source": [
    "***Plotting Out Top Developers***\n",
    "\n",
    "Here, a histogram is created to represent the proportion of contributions made by the top developers compared to the total number of commits to each project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GA-gqf4neGNZ"
   },
   "source": [
    "**First:** Count the number of commits for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d75tsY_hd_U8"
   },
   "outputs": [],
   "source": [
    "def count_repo_commit(project, uname, pword):\n",
    "    url = 'https://api.github.com/repos/%s/stats/contributors' % project\n",
    "\n",
    "    response = requests.get(url, auth=(uname, pword))\n",
    "    json_response = response.json()\n",
    "\n",
    "    count = 0\n",
    "    for item in json_response:\n",
    "        count += int(item['total'])\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aewzKQheXUa"
   },
   "source": [
    "**Second:** We create a list for the proportioned number of commits within the script. We then use this list to help create the bar chart. Code and chart shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "G7bxW9NEgX-T",
    "outputId": "5d8589cc-f005-478a-a4ef-ffff31eda539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub Authentication Username: aacosta13\n",
      "Password: ··········\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "'''\n",
    "Small script to retrieve domain champ info from different repos. Prints to file and creates a pandas dataframe\n",
    "\n",
    "By: Armando Acosta\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "project_dict = {\n",
    "    'Moose': 'idaholab/moose',\n",
    "    'Spack': 'spack/spack',\n",
    "    'yt': 'yt-project/yt',\n",
    "    'petsc': 'petsc/petsc',\n",
    "    'E3SM': 'E3SM-Project/E3SM',\n",
    "    'lammps': 'lammps/lammps',\n",
    "    'gromacs': 'gromacs/gromacs',\n",
    "    'OSGConnect': 'OSGConnect/TOREVIEW-tutorial-namd',\n",
    "    'QMCPACK': 'QMCPACK/qmcpack',\n",
    "    'Nek5000': 'Nek5000/Nek5000',\n",
    "    'nwchemgit': 'nwchemgit/nwchem',\n",
    "    # 'ECP-astro': '???', no url for this project\n",
    "    'lanl': 'lanl/LATTE',\n",
    "    'CRL': 'gridaphobe/CRL',\n",
    "    'enzo-project': 'enzo-project/enzo-dev'\n",
    "}\n",
    "\n",
    "project_list = ['Moose', 'Spack', 'yt', 'petsc', 'E3SM', 'lammps', 'gromacs', 'OSGConnect', 'QMCPACK', 'Nek5000',\n",
    "                'nwchemgit', 'lanl', 'CRL', 'enzo-project']\n",
    "\n",
    "\n",
    "def count_repo_commit(project, uname, pword):\n",
    "    url = 'https://api.github.com/repos/%s/stats/contributors' % project\n",
    "\n",
    "    response = requests.get(url, auth=(uname, pword))\n",
    "    json_response = response.json()\n",
    "\n",
    "    count = 0\n",
    "    for item in json_response:\n",
    "        count += int(item['total'])\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def retrieve_repo_info(uname, pword):\n",
    "    devfile = open('dev_info.txt', 'w')\n",
    "    devfile.write('Repository Name/Top Contributor/Number of Contributions')\n",
    "    print(file=devfile)\n",
    "\n",
    "    data = {'Repository Name': [],\n",
    "            'Top Contributor': [],\n",
    "            'Number of Contributions': [],\n",
    "            'Contribution Total for Repo': []}\n",
    "\n",
    "    commit_proportion = []\n",
    "    commit_proportion_dict = {}\n",
    "\n",
    "    for project in project_dict:\n",
    "        # Following block for writing data to file\n",
    "        print(' ', file=devfile)\n",
    "        url = 'https://api.github.com/repos/%s/contributors' % project_dict[project]\n",
    "        commit_count = count_repo_commit(project_dict[project], uname, pword)\n",
    "\n",
    "        response = requests.get(url, auth=(uname, pword))\n",
    "        json_response = response.json()\n",
    "\n",
    "        print(project, file=devfile)\n",
    "        print('Top Contributor:', json_response[0]['login'], file=devfile)\n",
    "        print('Contributions:', json_response[0]['contributions'], file=devfile)\n",
    "        print('-----------------------------------', file=devfile)\n",
    "\n",
    "        # Following block for creating dataframe\n",
    "        data['Repository Name'].append(project)\n",
    "        data['Top Contributor'].append(json_response[0]['login'])\n",
    "        data['Number of Contributions'].append(json_response[0]['contributions'])\n",
    "        data['Contribution Total for Repo'].append(commit_count)\n",
    "\n",
    "        if commit_count != 0:\n",
    "            commit_proportion.append(int(json_response[0]['contributions']) / commit_count)\n",
    "            commit_proportion_dict[project] = commit_proportion[-1]\n",
    "\n",
    "    # Sorting values to make Bar Chart\n",
    "    sorted_project_list = []\n",
    "    commit_proportion.sort(reverse=True)\n",
    "    for value in commit_proportion:\n",
    "        for current_project, proportion in commit_proportion_dict.items():\n",
    "            if value == proportion:\n",
    "                sorted_project_list.append(current_project)\n",
    "\n",
    "    # Creating chart\n",
    "    x_pos = [i for i, _ in enumerate(sorted_project_list)]\n",
    "    plt.bar(x_pos, commit_proportion, color='green', align='center')\n",
    "    plt.xlabel(\"Project\")\n",
    "    plt.ylabel(\"Proportion of Commits\")\n",
    "    plt.title(\"Proportion of Commits by Top Developer\")\n",
    "\n",
    "    plt.xticks(x_pos, sorted_project_list, size=6, rotation=-45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    username = input('GitHub Authentication Username: ')\n",
    "    password = getpass.getpass(prompt='Password: ')\n",
    "    retrieve_repo_info(username, password)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uy_BuuYw153"
   },
   "source": [
    "***Inspecting Files Touched by Top Contributors***\n",
    "\n",
    "The second half of this script inspects all files touched by these top contributors within the respective repositories. Due to the amount of contributions per user, this script may take quite a while to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OA6zFPq_3x_"
   },
   "source": [
    "**First:** Setup is similar to first half, however we now must juggle two instances of the API requests due to accessing each commit's respective JSON data. This can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91n1EWFCw156"
   },
   "outputs": [],
   "source": [
    "def retrieve_file_info(uname, pwd):\n",
    "    for project in project_dict:\n",
    "        print(' ', file=commitfile)\n",
    "        commit_url = 'https://api.github.com/repos/%s/commits' % project_dict[project]\n",
    "        contributor_url = 'https://api.github.com/repos/%s/contributors' % project_dict[project]\n",
    "\n",
    "        commit_response = requests.get(commit_url, auth=(uname, pwd))\n",
    "        commit_json_response = commit_response.json()\n",
    "        contributor_response = requests.get(contributor_url, auth=(uname, pwd))\n",
    "        contributor_json_response = contributor_response.json()\n",
    "\n",
    "        item_count = 0\n",
    "        page_no = 1\n",
    "        max_page = False\n",
    "        # Parse through JSON response of API for author\n",
    "        while not max_page:\n",
    "            for item in commit_json_response:\n",
    "                if item['author'] is None:\n",
    "                    item_count += 1\n",
    "                    continue\n",
    "\n",
    "                # If commit belongs to top dev, analyze the files here\n",
    "                if item['author']['login'] == contributor_json_response[0]['login']:\n",
    "                    individual_commit_url = 'https://api.github.com/repos/%s/commits' % project_dict[project] \\\n",
    "                                            + '/' + str(item['sha'])\n",
    "                    individual_commit_response = requests.get(individual_commit_url, auth=(uname, pwd))\n",
    "                    individual_json_response = individual_commit_response.json()\n",
    "\n",
    "                    print('-----------------------------------')\n",
    "                    print('Developer: ', contributor_json_response[0]['login'])\n",
    "                    print('')\n",
    "                    print(\"Files modified by commit:\")\n",
    "\n",
    "                    for file in individual_json_response['files']:\n",
    "                        print(file['filename'])\n",
    "                item_count += 1\n",
    "\n",
    "            if item_count == 30:\n",
    "                page_no += 1\n",
    "                commit_url += '?&page=%s' % str(page_no)\n",
    "                commit_response = requests.get(commit_url, auth=(uname, pwd))\n",
    "                commit_json_response = commit_response.json()\n",
    "                item_count = 0\n",
    "\n",
    "            else:\n",
    "                max_page = True\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xPGY4goBIB5"
   },
   "source": [
    "As noted, the code is a bit more complex than the previous work due to having to search within each commit contributed by the top developer. This means there are \"nested\" API requests performed in order to retrieve the correct information. The full script is provided below with a sample of the output (full output not shown, as it would be very large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "IKkAdxFYBlaM",
    "outputId": "a9c3bdc2-ba87-43b7-8bd8-736c132b6c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub Authentication Username: aacosta13\n",
      "Password: ··········\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3c30fd225273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GitHub Authentication Username: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Password: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mretrieve_file_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-3c30fd225273>\u001b[0m in \u001b[0;36mretrieve_file_info\u001b[0;34m(uname, pwd)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommit_json_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                         \u001b[0mitem_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "'''\n",
    "Script to analyze files touched by top developer of different project repos\n",
    "\n",
    "By: Armando Acosta\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import getpass\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "project_dict = {\n",
    "    'Moose': 'idaholab/moose',\n",
    "    'Spack': 'spack/spack',\n",
    "    'yt': 'yt-project/yt',\n",
    "    'petsc': 'petsc/petsc',\n",
    "    'E3SM': 'E3SM-Project/E3SM',\n",
    "    'lammps': 'lammps/lammps',\n",
    "    'gromacs': 'gromacs/gromacs',\n",
    "    'OSGConnect': 'OSGConnect/TOREVIEW-tutorial-namd',\n",
    "    'QMCPACK': 'QMCPACK/qmcpack',\n",
    "    'Nek5000': 'Nek5000/Nek5000',\n",
    "    'nwchemgit': 'nwchemgit/nwchem',\n",
    "    'lanl': 'lanl/LATTE',\n",
    "    'CRL': 'gridaphobe/CRL',\n",
    "    'enzo-project': 'enzo-project/enzo-dev'\n",
    "}\n",
    "\n",
    "\n",
    "def retrieve_file_info(uname, pwd):\n",
    "    commitfile = open('commit_info.txt', 'w')\n",
    "    commitfile.write('Repository Name/Top Committer/Commit Files Affected')\n",
    "    print(file=commitfile)\n",
    "\n",
    "    for project in project_dict:\n",
    "        # Will be used to keep track of number of times the main directory is modified\n",
    "        file_dict = {}\n",
    "\n",
    "        print(' ', file=commitfile)\n",
    "        commit_url = 'https://api.github.com/repos/%s/commits?&per_page=100' % project_dict[project]\n",
    "        contributor_url = 'https://api.github.com/repos/%s/contributors?&per_page=100' % project_dict[project]\n",
    "\n",
    "        commit_response = requests.get(commit_url, auth=(uname, pwd))\n",
    "        commit_json_response = commit_response.json()\n",
    "        contributor_response = requests.get(contributor_url, auth=(uname, pwd))\n",
    "        contributor_json_response = contributor_response.json()\n",
    "\n",
    "        item_count = 0\n",
    "        page_no = 1\n",
    "        max_page = False\n",
    "        # Parse through JSON response of API for author\n",
    "        while not max_page:\n",
    "            if commit_json_response is None:\n",
    "                max_page = True\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                for item in commit_json_response:\n",
    "                    if item['author'] is None:\n",
    "                        item_count += 1\n",
    "                        continue\n",
    "\n",
    "                # If commit belongs to top dev, analyze the files here\n",
    "                    if item['author']['login'] == contributor_json_response[0]['login']:\n",
    "                        individual_commit_url = 'https://api.github.com/repos/%s/commits' % \\\n",
    "                                                project_dict[project] + '/' + str(item['sha'])\n",
    "                        individual_commit_response = requests.get(individual_commit_url, auth=(uname, pwd))\n",
    "                        individual_json_response = individual_commit_response.json()\n",
    "\n",
    "                        for file in individual_json_response['files']:\n",
    "                            main_directory = file['filename'].split('/')\n",
    "\n",
    "                            if main_directory[0] in file_dict:\n",
    "                                file_dict[main_directory[0]] += 1\n",
    "                            else:\n",
    "                                file_dict[main_directory[0]] = 1\n",
    "\n",
    "                    item_count += 1\n",
    "\n",
    "                if item_count == 100:\n",
    "                    page_no += 1\n",
    "                    commit_url = 'https://api.github.com/repos/%s/commits?&per_page=100' % project_dict[project]\n",
    "                    commit_url += '&page=%s' % str(page_no)\n",
    "                    commit_response = requests.get(commit_url, auth=(uname, pwd))\n",
    "                    commit_json_response = commit_response.json()\n",
    "                    item_count = 0\n",
    "                else:\n",
    "                    max_page = True\n",
    "\n",
    "        # Setting properties for pie (exploding max value, colors, wedge properties, etc)\n",
    "        explode = [0.0] * len(file_dict.values())\n",
    "        value_list = list(file_dict.values())\n",
    "        max_value_index = value_list.index(max(value_list))\n",
    "        explode[max_value_index] = 0.1\n",
    "        wp = {'linewidth': 1, 'edgecolor': \"black\"}\n",
    "\n",
    "        # Creating and plotting pie chart\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.pie(file_dict.values(), explode=explode, labels=file_dict.keys(), autopct='%.2f', shadow=True,\n",
    "                startangle=90, wedgeprops=wp)\n",
    "        ax1.axis('equal')\n",
    "\n",
    "        plt.title('Proportion of Files Modified by All Commits' + project + item['author'])\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    username = input('GitHub Authentication Username: ')\n",
    "    password = getpass.getpass(prompt='Password: ')\n",
    "    retrieve_file_info(username, password)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZqaU7yBPsHl"
   },
   "source": [
    "##**Overhelping Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyWwr5LOTFCc"
   },
   "source": [
    "***Multiple Replies***\n",
    "\n",
    "This part of the script analyzes pull requests that encounter multiple comments attached to the request, implying an excess of development help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nShsByM8ZCgT"
   },
   "source": [
    "**First:** Like the previous scripts, this one follows the same convention of using the API to collect the information of each pull request. Like the second half of the Domain Champion script, a method of \"nested\" API requests are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3ZArqTOZrrl"
   },
   "outputs": [],
   "source": [
    "def retrieve_pull_info(uname, pwd):\n",
    "    for project in project_dict:\n",
    "        pull_url = 'https://api.github.com/repos/%s/pulls' % project_dict[project]\n",
    "        pull_response = requests.get(pull_url, auth=(uname, pwd))\n",
    "        pull_json_response = pull_response.json()\n",
    "\n",
    "        item_count = 0\n",
    "        page_no = 1\n",
    "        max_page = False\n",
    "        # Parse through JSON response of API for comments list\n",
    "        while not max_page:\n",
    "            for item in pull_json_response:\n",
    "                # Skip if no comments url\n",
    "                if item['review_comments_url'] is None:\n",
    "                    item_count += 1\n",
    "                    continue\n",
    "\n",
    "                # If comments url exists jump to it\n",
    "                comments_url = item['review_comments_url']\n",
    "                comments_response = requests.get(comments_url, auth=(uname, pwd))\n",
    "                comments_json_response = comments_response.json()\n",
    "                comment_count = 0\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJY-CF0oZ6_H"
   },
   "source": [
    "Upon running the completed script, one will see the comments of the \"overcommented\" pull requests printed to the terminal along with the comment count. A sample may be seen below (work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8fi2TtbkaOHO",
    "outputId": "f4bef35d-f191-495a-ae2c-e034d2d013ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub Authentication Username: aacosta13\n",
      "Password: ··········\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Total comments for pull:  30\n",
      "-----------------------\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Total comments for pull:  11\n",
      "-----------------------\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Repository:  Moose\n",
      "Pull comment:  **Caution!** This contains a submodule update\n",
      "Total comments for pull:  4\n",
      "-----------------------\n",
      "Repository:  Moose\n",
      "Pull comment:  Were you able to try out the analytic computation of mean and std for the PC models?\n",
      "Repository:  Moose\n",
      "Pull comment:  Yes I tried that first but then I wanted to get confidence intervals and switched to the other type. The difference was minimal because we had 100,000 samples for 3 parameters. \n",
      "Total comments for pull:  4\n",
      "-----------------------\n",
      "Repository:  Moose\n",
      "Pull comment:  OK, my point is that: if do ` mesh->getMesh().allow_remote_element_removal(false)` in my mesh generator, then I would like to keep all remotes regardless of RMs.  That is all I want, but it is just so hard\n",
      "Repository:  Moose\n",
      "Pull comment:  I like something like: `force_no_remote_removal ` :-)\n",
      "Repository:  Moose\n",
      "Pull comment:  Yikes yea that gets a bit dicey. In the code I pasted above we want to reenable remote element deletion *if* remote element deletion was disabled by the `Action`, but *not* if remote element deletion was disabled by a user's mesh generator.\n",
      "\n",
      "I'm curious why you never want to allow remote element removal?\n",
      "Repository:  Moose\n",
      "Pull comment:  At that point why use a `DistributedMesh`?\n",
      "Repository:  Moose\n",
      "Pull comment:  Do you do some initial remote element deletion? Or are you reading in a split mesh?\n",
      "Repository:  Moose\n",
      "Pull comment:  I am in distributed mesh generator ....\n",
      "Repository:  Moose\n",
      "Pull comment:  > I'm curious why you never want to allow remote element removal?\n",
      "\n",
      "I'm still curious about the answer to this question.\n",
      "Repository:  Moose\n",
      "Pull comment:  I will add RMs so that we can get rid of this fragile parameter.\n",
      "Repository:  Moose\n",
      "Pull comment:  Now that you are serializing the displaced mesh for exodus output, do you still need this change? What tests fail if you don't have it? I am worried that we will forget to remove this code in the future and a lot of our meshes will be serial without even realizing it.\n",
      "\n",
      "Additionally, I am curious what tests fail if you don't have your `allow_remote_element_removal(false)` in your `DistributedMeshGenerator`. I know that you put it there for a reason...something to do with periodic boundary conditions I believe.\n",
      "Repository:  Moose\n",
      "Pull comment:  > Additionally, I am curious what tests fail if you don't have your `allow_remote_element_removal(false)` in your `DistributedMeshGenerator`. I know that you put it there for a reason...something to do with periodic boundary conditions I believe.\n",
      "\n",
      "Periodic boundary tests will fail. The reason is that:\n",
      "\n",
      " After I have done mesh generation, a `prepare_for_use` will be involved inside of mesh generator, but the mesh generator has no idea whether periodic BCs will be used later.  Libmesh will remove elements that are needed for periodic BCs later. \n",
      "Repository:  Moose\n",
      "Pull comment:  The fix would happen if we know we should keep for the later simulation. It has to be implemented by running `add_rm` before `mesh generation`.\n",
      "\n",
      "I will do a bit of exploration.\n",
      "Repository:  Moose\n",
      "Pull comment:  I do not need `_mesh->getMesh().allow_remote_element_removal()` anymore once RMs are setup correctly.\n",
      "Total comments for pull:  14\n",
      "-----------------------\n",
      "Repository:  Moose\n",
      "Pull comment:  Yeah - I would say remove it unless we have a specific reason.\n",
      "\n",
      "I think the bigger concern is the gfortran ones which may be causing BISON to fail.\n",
      "Total comments for pull:  3\n",
      "-----------------------\n",
      "Repository:  Moose\n",
      "Pull comment:  Wait, what?\n",
      "Repository:  Moose\n",
      "Pull comment:  What you can't understand my late night ramblings? :)\n",
      "\n",
      "I have a trying-to-be converted to AD kernel that takes derivative from the material created by `DerivativeSumMaterial`. I tried taking the derivatives via `getMaterialPropertyDerivative<Real, true>(_derivative_sum_material_name, _var.name())`, but that always returns zero. When I use `getADMaterialProperty<Real>(derivativePropertyNameFirst(_linked_mat_names[i], _var.name()))` (based off of `ADACInterface`), I get `Material property 'df_name/dvar', requested by 'somekernel' is not defined on block 0`, so it seems like `DerivativeSumMaterial` is not creating all the derivatives it needs. If this is too confusing, I can send you a source file...\n",
      "Repository:  Moose\n",
      "Pull comment:  Uuuuaahhhh, how does any of this work?! How did the unit test I wrote even work?! I'll get on this.\n",
      "Repository:  Moose\n",
      "Pull comment:  Actually, my problem isn't with `ADDerivativeSumMaterial`, it's with trying to pull derivatives as I explained above. I was able to double check this by just making a ADDerivativeParsedMaterial` that did the same thing as `ADDerivativeSumMaterial`, it seems like it is deeper with how derivatives are being pulled?\n",
      "Repository:  Moose\n",
      "Pull comment:  OH I got it!! When doing `derivativePropertyNameFirst`, It doesn't do a name look up from the inputparams like getMaterialPropertyDerivative does!\n",
      "Repository:  Moose\n",
      "Pull comment:  You may be well on your way here anyways, but just a note that `getADMaterialProperty<Real>(derivativePropertyNameFirst(\"some_derivative_parsed_material\", \"some_var\"))` will return an error instead of a zero if \"some_derivative_parsed_material\" doesn't depend on \"some_var\".\n",
      "Repository:  Moose\n",
      "Pull comment:  Ok, after taking a closer look, the `TODO: declareGenericPropertyDerivative` should have been removed, because `declarePropertyDerivative<Real, is_ad>` does just that. I'll switch to looking at the defaultMaterialProperty stuff\n",
      "Repository:  Moose\n",
      "Pull comment:  Yeah exactly. It mostly works, just the small helper details\n",
      "Repository:  Moose\n",
      "Pull comment:  Oooof...that startup is suppppper slow now with derivatives of AD...\n",
      "Repository:  Moose\n",
      "Pull comment:  every time or just the first compilation?\n",
      "Repository:  Moose\n",
      "Pull comment:  Well first time, and then JIT kicks in...but remember I'm running thousands of these things...stochastic tools would help\n",
      "Repository:  Moose\n",
      "Pull comment:  I am running with sparse matrix...would it be better to be non-sparse?\n",
      "Repository:  Moose\n",
      "Pull comment:  AD with the derivatives is unusable for the very large problems\n",
      "Total comments for pull:  15\n",
      "-----------------------\n",
      "Repository:  Moose\n",
      "Pull comment:  From the picture, it seems we should use `u1c`\n",
      "Repository:  Moose\n",
      "Pull comment:  OK, you simply swap `1` and `2`, haha\n",
      "Repository:  Moose\n",
      "Pull comment:  No close\n",
      "Repository:  Moose\n",
      "Pull comment:  No close\n",
      "Repository:  Moose\n",
      "Pull comment:  My apologies for the confusion. In the code I use \"1\" as the primal and \"2\" as the secondary... Just to match the naming. While in the figure it is the opposite. I will update the figure for your easier understanding\n",
      "Repository:  Moose\n",
      "Pull comment:  > No close\n",
      "\n",
      "Why is that? I thought I should close the matrix every time I am done with changing the values? \n",
      "Repository:  Moose\n",
      "Pull comment:  We could simply scale _M to get MD since D is a vector \n",
      "Repository:  Moose\n",
      "Pull comment:  No close\n",
      "Repository:  Moose\n",
      "Pull comment:  Please refer to this figure instead @fdkong while reviewing the code\n",
      "![Screen Shot 2020-07-10 at 12 16 19 PM](https://user-images.githubusercontent.com/58825741/87185604-4b7c8480-c2a7-11ea-94b2-b5f4f2a10b7f.png)\n",
      "\n",
      "Repository:  Moose\n",
      "Pull comment:  Cool! It is really helpful. \n",
      "Repository:  Moose\n",
      "Pull comment:  If we do `_matrix->create_submatrix(* _D, u2c, lm)`, we do not this transpose anymore\n",
      "Repository:  Moose\n",
      "Pull comment:  ` _matrix->create_submatrix(* _M, u1c, lm);`\n",
      "Repository:  Moose\n",
      "Pull comment:  You need to manually close only when you have `MatSetValues`. `create_submatrix` and `get_transpose` already take care of close \n",
      "Repository:  Moose\n",
      "Pull comment:  Oh I remember here is a reason why I did not directly extract the sub matrix but choose to do the transpose (which is not optimal):\n",
      "\n",
      " The Jacobian is after applying BC, where the D block has some rows set to zero, similar with M. No BC is applied to the rows corresponding to the LMs, so I am extracting transpose(D), and transpose(M) first. \n",
      "\n",
      "I was thinking It would probably make more sense to do the condensing before applying the BC, but that probably will be really complex and does not bring much benefit\n",
      "Repository:  Moose\n",
      "Pull comment:  The reason was basically the Jacobian (i.e., _matrix) is not symmetric anymore after applying the BC, where the entries of D and M are changed. However, since the rows associated with LM dofs are not touched, I am extracting D^T and M^T from there, and do the transpose to retrieve D and M\n",
      "Repository:  Moose\n",
      "Pull comment:  Should `lm` be replaced with `u2c`?\n",
      "Repository:  Moose\n",
      "Pull comment:  Response to your comments:\n",
      "\n",
      "You do not need to initialize `MDinvK2cc`, and it will be taken care of by `create_submatrix`.\n",
      "\n",
      "\n",
      "Repository:  Moose\n",
      "Pull comment:  We need to somehow preallocate `_J_condensed ` here.\n",
      "\n",
      "My thought is: \n",
      "\n",
      "1) compare the sparsity of `MDinvK2ci`  with that of `K2ci`,  \n",
      "\n",
      "2) Retrieve the sparsity of `_J_condensed` by looping over rows.\n",
      "\n",
      "3) Appending  differences, we recreate a matrix `_J_condensed_new`\n",
      "\n",
      "4) `_J_condensed_new-> add_sparse_matrix(_J_condensed)`\n",
      "Repository:  Moose\n",
      "Pull comment:  Did you mean the interface may have an intersection with actual boundaries? And then a Dirichlet BCs will D and M because of some common mesh points between the interface and the boundaries.\n",
      "Repository:  Moose\n",
      "Pull comment:  No. The row are all dofs except for the u2c, the col are all dofs except for lm.\n",
      "Repository:  Moose\n",
      "Pull comment:  > Response to your comments:\n",
      "> \n",
      "> You do not need to initialize `MDinvK2cc`, and it will be taken care of by `create_submatrix`.\n",
      "\n",
      "Thanks! Here I actually like an all-zero `MDinvK2cc`. The actual values are computed later from `_MDinv->matrix_matrix_mult(*_K2ci, *MDinvK2ci);` I do not see an all-zero-initializer for PetscMatrix. This is why I am calling `crate_submatrix`.. I think this should be changed once we have a proper initializer\n",
      "Repository:  Moose\n",
      "Pull comment:  Would you mind pointing to me where the matrix sparsity information are? Seems to be an important component here that I have no idea about...\n",
      "Repository:  Moose\n",
      "Pull comment:  > Did you mean the interface may have an intersection with actual boundaries? And then a Dirichlet BCs will D and M because of some common mesh points between the interface and the boundaries.\n",
      "\n",
      "Yes, you are right.\n",
      "Repository:  Moose\n",
      "Pull comment:  I agree it would be better to do the scale! However, I do not see such a function in the class. Is there?\n",
      "Total comments for pull:  26\n",
      "-----------------------\n",
      "Repository:  Moose\n",
      "Pull comment:  ```suggestion\n",
      "```\n",
      "Repository:  Moose\n",
      "Pull comment:  ```suggestion\n",
      "  const Real & _delta;\n",
      "  const unsigned int & _j;\n",
      "  const Real & _theta0;\n",
      "  const Real & _eps_bar;\n",
      "```\n",
      "Repository:  Moose\n",
      "Pull comment:  ```suggestion\n",
      "  params.addRequiredParam<Real>(\"coef\", \"Scaling term applied to the time derivative.\");\n",
      "```\n",
      "Repository:  Moose\n",
      "Pull comment:  oops, that issn \"fix\" is wrong, don't do that little guy\n",
      "\n",
      "Total comments for pull:  6\n",
      "-----------------------\n",
      "Repository:  Spack\n",
      "Pull comment:  ```suggestion\n",
      "        if '^fftw@3:' in self.spec:\n",
      "            options.append('-DGMX_FFT_LIBRARY=fftw3')\n",
      "        elif '^intel-mkl' in self.spec:\n",
      "            options.append('-DGMX_FFT_LIBRARY=mkl')\n",
      "\n",
      "```\n",
      "Repository:  Spack\n",
      "Pull comment:  Thanks. Your solution is much simpler, although technically it is two different interfaces that are used by gromacs (there is actually a third - fftpack, which is builtin). Furthermore, for cuda builds neither fftw nor mkl seems to be needed http://manual.gromacs.org/documentation/current/install-guide/index.html#fast-fourier-transform-library , but I haven't used or tried to compile it . I will put in your changes and use a `when='~cuda'` for the dependency. Maybe the maintainers can comment on that.\n",
      "Total comments for pull:  4\n",
      "-----------------------\n",
      "Total comments for pull:  2\n",
      "-----------------------\n",
      "Repository:  Spack\n",
      "Pull comment:  Yes, that seems necessary on some Linux OS while others don't seem to require it. Safer to just always specify it.\n",
      "Total comments for pull:  3\n",
      "-----------------------\n",
      "Repository:  Spack\n",
      "Pull comment:  Do all of these dependencies need to be these exact versions, or is that the lowest acceptable version? If the latter, add a colon to the end to create an open-ended version range.\n",
      "Repository:  Spack\n",
      "Pull comment:  Need docstring description\n",
      "Repository:  Spack\n",
      "Pull comment:  The way that Spack's version sorting works, 2.3rc3 is actually newer than 2.3 and will be installed by default. Either drop 2.3rc3 if you don't need it, or do:\n",
      "```suggestion\n",
      "    version('2.3', sha256='fc47070e2e9fac09b97022be2320200d732a0a4a820a2b51532b88f8ded14536', preferred=True)\n",
      "    version('2.3rc3', sha256='85a9f1ea1a837d487e356f021ef6f3a4661ad270a0c5f54777b362ee4d45166f')\n",
      "```\n",
      "Repository:  Spack\n",
      "Pull comment:  I was forced to add a \"wrapper\" around the cuda variant because I didn't find a way to list the dependencies of the binaries that were available in `$ spack buildcache list`\n",
      "\n",
      "`$ spack find` has a `-d` option so the user can know the version of dependencies but i found no equivalent for `spack buildcache list`\n",
      "\n",
      "So to be able to specify what the allowed values of cuda are(the versions that we have generated binaries for), I added the redundant variant.\n",
      "\n",
      "This way, I was able to add a description in the variant field, so users will know what cuda versions are acceptable in `$ spack info`\n",
      "\n",
      "\n",
      "Repository:  Spack\n",
      "Pull comment:  If the maintainers of the dependency update their package, (eg. bison updated theirs recently), the hash value of MVAPICH2-X and MVAPICH2-GDR binaries would become outdated and `$spack install` would fail to find them on the binary mirror unless the user specifies the exact version of the dependency that the binaries were created with. \n",
      "\n",
      "This is because the hashes would now be generated based on the new version of the dependencies.\n",
      "Repository:  Spack\n",
      "Pull comment:  You can specify supported versions with:\n",
      "```python\n",
      "depends_on('cuda@9.2:10.2')\n",
      "```\n",
      "As for the buildcache issue, feel free to open a separate issue for that. @gartung may be able to quickly add the feature you need.\n",
      "Repository:  Spack\n",
      "Pull comment:  Yes, this is true for every single package in Spack. But if every package chooses a random version to link to, you won't be able to have a single DAG with a single version of a library.\n",
      "\n",
      "There is ongoing work on a new concretizer that will consider already available binary versions before choosing which version to install. Until then, you should allow the dependency to choose as wide of a range of versions as possible.\n",
      "Repository:  Spack\n",
      "Pull comment:  Use `spack buildcache list -lv` to list the buildcaches available including the variants used and the hash.\n",
      "Use `spack buildcache install /hash` to install the package and all of it's link and run dependencies it was built with.\n",
      "The hash is shorthand for the full spec.\n",
      "Repository:  Spack\n",
      "Pull comment:  @gartung We have created our binaries with `spack buildcache create --only package`, so spack buildcache install /hash fails for me. Is there any way to get it to install dependencies from source but the binary from the mirror using only `$ spack buildcache install /hash`\n",
      "Repository:  Spack\n",
      "Pull comment:  Thanks, I understand the reasoning behind your suggestions. But, I did face the following issue which required the addition of explicit versions of dependencies.\n",
      " \n",
      "When we created our binaries, they were dependent on the latest versionof bison, at that time - 3.4.2 `depends_on('bison')`. However, after a certain date, the `spack install` command stopped working for all binaries in our mirror. And we found that this was due to the update of the bison package which now has 3.6.4 as its latest version.\n",
      "\n",
      "So, unless users specify `^bison@3.4.2`, spack install will not find any of the binaries in our mirror. How would we resolve this without creating new binaries for each version of bison?\n",
      "Repository:  Spack\n",
      "Pull comment:  That can't be fixed unless you also created buildcaches for the dependencies.\n",
      "Repository:  Spack\n",
      "Pull comment:  Hmm, I would add binaries for the latest version of bison, that's what Spack will install by default. Once the new concretizer comes out (next Spack release) it will be smart enough to use the binary version even if it's older.\n",
      "Total comments for pull:  14\n",
      "-----------------------\n",
      "Total comments for pull:  2\n",
      "-----------------------\n",
      "Repository:  Spack\n",
      "Pull comment:  does spack have special support for `master`, could that support also include `main`?\n",
      "Repository:  Spack\n",
      "Pull comment:  `master` branch no longer exists ...\n",
      "Repository:  Spack\n",
      "Pull comment:  We can name the version whatever we want. We currently have support for `develop`, `master`, `head`, and `trunk`. We could add `main`, or we could do:\n",
      "```suggestion\n",
      "    version('master', branch='main', submodules=True)\n",
      "```\n",
      "Repository:  Spack\n",
      "Pull comment:  Spoke to @becker33, added fix from him that added main to the list of blessed branches.\n",
      "Total comments for pull:  6\n",
      "-----------------------\n",
      "Repository:  yt\n",
      "Pull comment:  ```suggestion\n",
      "        Registers a subclass at creation.\n",
      "```\n",
      "Total comments for pull:  3\n",
      "-----------------------\n",
      "Repository:  yt\n",
      "Pull comment:  ```suggestion\n",
      "                \"Unexpected output from test OpenMP program (output was %s)\", output\n",
      "```\n",
      "Repository:  yt\n",
      "Pull comment:  ```suggestion\n",
      "                \"Warning: Invalid value of disparity; now reset it to %f\",\n",
      "```\n",
      "Total comments for pull:  4\n",
      "-----------------------\n",
      "Total comments for pull:  2\n",
      "-----------------------\n",
      "Repository:  yt\n",
      "Pull comment:  so do you know what would be the defaults here ? or if it is in fact possible that this Nyx simulation is not cosmological ?\n",
      "Repository:  yt\n",
      "Pull comment:  I'm really not a Nyx person, but @jmsexton03 is.  Maybe she can answer this?\n",
      "Repository:  yt\n",
      "Pull comment:  I don't know what `nyx_small` is, I do know that the job_info files from Nyx plotfiles I generate will have those cosmology fields in them. I'm not clear on what's broken here.\n",
      "Repository:  yt\n",
      "Pull comment:  looking at that file, it looks like it was produced in `Nyx/Exec/simple_output`, but that directory no longer exists in Nyx.  The file was originally output in 2012, so a lot must have changed.\n",
      "Repository:  yt\n",
      "Pull comment:  Nyx now also includes an omega_radiation cosmology term (almost always 0). If we add that number to job_info in a similar way, can yt store it in the same data structure?\n",
      "https://github.com/yt-project/yt/blob/fdc9210a35e41b45c114f271798ea46c2179d2eb/yt/utilities/cosmology.py#L41\n",
      "Repository:  yt\n",
      "Pull comment:  Sounds like yt is very unlikely to ever come across these outdated formats then. I can probably just make this a special case with `cosmological_simulation = 0`\n",
      "Repository:  yt\n",
      "Pull comment:  indeed, I suspect a lot of the sample data is quite outdated.  Maybe we should have a push (separate from this PR) to update some of the datasets to current code output.\n",
      "Repository:  yt\n",
      "Pull comment:  Nyx simulations have had cosmology fields in job_info since 2016 when Nyx was first on github, so it's highly unlikely a Nyx plotfile without cosmology fields in job_info will be used now. It looks like your fix should handle it well.\n",
      "Total comments for pull:  10\n",
      "-----------------------\n",
      "Total comments for pull:  2\n",
      "-----------------------\n",
      "Repository:  yt\n",
      "Pull comment:  This is the part addressed originally in #2787\n",
      "Repository:  yt\n",
      "Pull comment:  The default in AMReX is `False` if not specified: https://amrex-codes.github.io/amrex/docs_html/InputsProblemDefinition.html\n",
      "```suggestion\n",
      "        self.periodicity = [False] * 3\n",
      "```\n",
      "Repository:  yt\n",
      "Pull comment:  This line is the crash that #2787 fixes.\n",
      "Repository:  yt\n",
      "Pull comment:  Thank you for checking this ! I assumed the dominant behavior was correct but I guess that's one more bug to take down here. Will fix as soon as I get my hands on my laptop.\n",
      "Repository:  yt\n",
      "Pull comment:  I went back and applied the same fix to BoxlibDataset itself as well as Castro following the docs.\n",
      "Repository:  yt\n",
      "Pull comment:  @matthewturk can I have your review on this comment you wrote ? I don't know what was there to fix (default value, type, anything else ?) and if that comment should be cleaned up or updated.\n",
      "Total comments for pull:  8\n",
      "-----------------------\n",
      "Repository:  yt\n",
      "Pull comment:  btw, I know it's not your fault, but this line is not only useless but also harmful since it overwrites the original type for this attribute (`MutableAttribute`).\n",
      "Sorry you happened to touch such a buggy bunch of lines !\n",
      "Repository:  yt\n",
      "Pull comment:  @neutrinoceros are we using this correctly? :)\n",
      "```\n",
      "    self.periodicity[:] = False\n",
      "TypeError: 'tuple' object does not support item assignment\n",
      "```\n",
      "Repository:  yt\n",
      "Pull comment:  Yup, apparently in this particular class we're forcing the tuple type, but there's also a note that says it should be fixed...\n",
      "https://github.com/yt-project/yt/blob/9138a0a7330e642e040f8dc0a459f7dba347fa05/yt/frontends/boxlib/data_structures.py#L624\n",
      "\n",
      "Anyway, this is a different problem than the one you're fixing, sorry again... I'll fix this in a separate branch and if all goes well we'll be able to merge this one right after.\n",
      "Repository:  yt\n",
      "Pull comment:  So it's much worse than I anticipated, and there is a lot of inconsistency right now in how we handle this particular attribute across frontends. You may simply disregard this and backpedal to using tuples. I'm sorry for the back and forth, thanks for your reactivity.\n",
      "Repository:  yt\n",
      "Pull comment:  I think I will set it to `True` as other readers in that frontend and ignore it altogether. This will cause the least breakage.\n",
      "Repository:  yt\n",
      "Pull comment:  This won't work either because tuples are immutable, here's a fix (backpedaling again...)\n",
      "```suggestion\n",
      "        self.periodicity = [False, False, False]\n",
      "        if \"geometry.is_periodic\" in self.parameters:\n",
      "            is_periodic = self.parameters[\"geometry.is_periodic\"].split()\n",
      "            self.periodicity[: len(is_periodic)] = [val == \"1\" for val in is_periodic]\n",
      "        self.periodicity = ensure_tuple(self.periodicity)\n",
      "```\n",
      "Repository:  yt\n",
      "Pull comment:  applied as code comment.\n",
      "Total comments for pull:  9\n",
      "-----------------------\n",
      "Repository:  yt\n",
      "Pull comment:  Do you think it helps test something that isn't in other tests? Or? \n",
      "Total comments for pull:  3\n",
      "-----------------------\n",
      "Repository:  yt\n",
      "Pull comment:  Well, I'll say that it's not your fault, I just have mixed feelings with encouraging folks to supply a normal vector.  I know that internally it's the same, but I think we made it that way so that you could swap out `\"x\"` and `[0.2, 0.3, 0.4]` or something without changing the argument name.\n",
      "Repository:  yt\n",
      "Pull comment:  So you mean this argument should be positional ? In python 3.8 positional-only arguments are a thing, but in the mean time, a different approach could be to remove the default value for normal, making it mandatory and encouraging the positional-arg syntax. The trade-off here is that it can not be done while also supporting backward compatibility.\n",
      "While I'm all about removing dead code where I can, it was never signaled to users that the old behaviour was discouraged/deprecated, hence my choice of going for a smooth deprecation path. What do you think ?\n",
      "Repository:  yt\n",
      "Pull comment:  If I read the code correctly, it still is possible to call `yt.SlicePlot(ds, whatever, 'density')` with `whatever='x'` or `whatever=[0.2,0.3,0.4]`. @neutrinoceros am I wrong?\n",
      "\n",
      "Repository:  yt\n",
      "Pull comment:  Absolutely ! (no, you're not wrong)\n",
      "I'm only trying to handle the case where the keyword `axis` is used instead of `normal`, but for scripts that write them positionally, it shouldn't change anything !\n",
      "Repository:  yt\n",
      "Pull comment:  So I'm ok with the way it is coded right now. @matthewturk is it good to go for you? I'm ok to merge\n",
      "Total comments for pull:  7\n",
      "-----------------------\n",
      "Repository:  yt\n",
      "Pull comment:  of course ! I'm taking a bet here :)\n",
      "Total comments for pull:  3\n",
      "-----------------------\n",
      "Repository:  yt\n",
      "Pull comment:  Wouldn't this test be closer to user behavior if it wasn't passed through `Path`? \n",
      "Repository:  yt\n",
      "Pull comment:  A numpy array or a tuple in particular should work already. Though I can't think of any other useful iterable, I think ALL iterables should be supported already, given the way the first line is written.\n",
      "Repository:  yt\n",
      "Pull comment:  makes sense, I'll update it !\n",
      "Repository:  yt\n",
      "Pull comment:  Do you mean something like this ?\n",
      "```python\n",
      "    filepath = Path(blast_wave_parfile)\n",
      "    read_amrvac_namelist(filepath)\n",
      "```\n",
      "Repository:  yt\n",
      "Pull comment:  I was meaning that I think a user is more likely to do something like `read_amrvac_namelist(blast_wave_parfile)` without using `Path` at all. \n",
      "Repository:  yt\n",
      "Pull comment:  Of course they are, but this test exist solely to check that they (... I 🙈) can do it.\n",
      "Repository:  yt\n",
      "Pull comment:  done !\n",
      "Total comments for pull:  9\n",
      "-----------------------\n",
      "Repository:  yt\n",
      "Pull comment:  I'm not a big fan either. Maybe we should throw an error instead (or maybe just a runtime warning ?) in the non-cartesian cases and warn the user they should set `to_array=False`... wdyt ?\n",
      "Repository:  yt\n",
      "Pull comment:  I believe throwing an error encouraging to use `to_array` would be better, as most yt user I encountered do not even read the warnings anyway.\n",
      "Repository:  yt\n",
      "Pull comment:  I added an error and docstrings !\n",
      "Repository:  yt\n",
      "Pull comment:  Ah, of course this is breaking a lot of tests on Jenkins... which probably means potentially a lot of downstream code... @cphyc, @matthewturk  do you think this is worth the break ?\n",
      "Repository:  yt\n",
      "Pull comment:  If that's breaking a lot of downstream code, I'm also ok with issuing a warning.\n",
      "Repository:  yt\n",
      "Pull comment:  adressed !\n",
      "Total comments for pull:  8\n",
      "-----------------------\n",
      "Repository:  yt\n",
      "Pull comment:  This function is a direct copy of `IOHandlerGadgetFOFHaloHDF5._read_particle_selection`. The original lost version of this PR included a refactor which merged them. I intend to re-do that refactor at some point, but I think it would be easier to do after this PR has been merged. I appeal for clemency until then.\n",
      "\n",
      "I'm also happy to (for now) replace this function definition explicitly with:\n",
      "```\n",
      "_read_particle_selection = IOHandlerGadgetFOFHaloHDF5._read_particle_selection\n",
      "```\n",
      "That would at least cut down on code to review and make it clear what work will be done in the future.\n",
      "Total comments for pull:  3\n",
      "-----------------------\n",
      "Repository:  yt\n",
      "Pull comment:  @matthewturk no, unfortunately the original way doesn't work correctly either. I think we have to resolve it now. \n",
      "Repository:  yt\n",
      "Pull comment:  Never mind, I figured this out!!\n",
      "Total comments for pull:  4\n",
      "-----------------------\n",
      "Repository:  E3SM\n",
      "Pull comment:  thanks, will add\n",
      "Repository:  E3SM\n",
      "Pull comment:  It seems that `use_fates_spitfire` has been replaced by `fates_spitfire_mode`. If true, can `use_fates_spitfire` be deleted?\n",
      "Repository:  E3SM\n",
      "Pull comment:  yes, good catch @bishtgautam \n",
      "Repository:  E3SM\n",
      "Pull comment:  updated code\n",
      "Total comments for pull:  6\n",
      "-----------------------\n",
      "Total comments for pull:  2\n",
      "-----------------------\n",
      "Repository:  E3SM\n",
      "Pull comment:  Also, shouldn't something this generic be in a separate \"radiation_utils.F90\" module?\n",
      "Repository:  E3SM\n",
      "Pull comment:  In the event I need a 2d version I'd overload it. Prefer to keep the name as-is for now until we need that, because the interface would stay the same.\n",
      "Repository:  E3SM\n",
      "Pull comment:  RRTMGP does this internally anyways. This was just an extra check added at some point in the debugging process to catch where things were going awry.\n",
      "Total comments for pull:  5\n",
      "-----------------------\n",
      "Repository:  E3SM\n",
      "Pull comment:  Right, but this part is not within an ifdef, so whether or not that section below gets triggered by running with a non-theta-l configuration, the values of p_i will already be calculated, so no need to calculate them twice. \n",
      "Total comments for pull:  3\n",
      "-----------------------\n",
      "Repository:  E3SM\n",
      "Pull comment:  se_tstep was already set above\n",
      "Repository:  E3SM\n",
      "Pull comment:  this block of XML breaks our convention of preqx using the old interface and theta-l using the new interface.   just wanted to make sure it was done intentionally.   \n",
      "Repository:  E3SM\n",
      "Pull comment:  I originally had different values, I'll remove these. \n",
      "Repository:  E3SM\n",
      "Pull comment:  Good point. \n",
      "So Maybe I should just add a line like this?\n",
      "  <nu dyn_target=\"theta-l\"> 3.4e-8</nu>\n",
      "That matches the value used for RRM, so it wouldn't affect that configuration. \n",
      "Repository:  E3SM\n",
      "Pull comment:  good, I'll move these lines around t be consistent with how everything is grouped. \n",
      "Repository:  E3SM\n",
      "Pull comment:  Yea, this was done intentionally to make it clear that the \"old\" style defaults are being overridden for ne45, but I can move them to be more consistent.\n",
      "Repository:  E3SM\n",
      "Pull comment:  After some offline discussion I decided to leave it in for now, and maybe later fix up this section to have a standard scaling anytime the tensor hyperviscosity is used. \n",
      "Total comments for pull:  9\n",
      "-----------------------\n",
      "Repository:  E3SM\n",
      "Pull comment:  Modify the if-else statements as mentioned in the above comment.\n",
      "Repository:  E3SM\n",
      "Pull comment:  Please delete the commentted out lines \n",
      "Repository:  E3SM\n",
      "Pull comment:  There are a couple of instances of `!++ams` in this PR, which I assume is developer's initials. If yes, please get rid of it as git blame will identify the author of the commit.\n",
      "Repository:  E3SM\n",
      "Pull comment:  Is this redundant and should be removed?\n",
      "Repository:  E3SM\n",
      "Pull comment:  Should this be removed?\n",
      "Repository:  E3SM\n",
      "Pull comment:  Here the code is:\n",
      "\n",
      "```\n",
      "if (.not. use_extrasnowlayers) then\n",
      "   ! 469 lines of code\n",
      "else\n",
      "  ! 133 lines of code\n",
      "end if\n",
      "```\n",
      "\n",
      "Given the extremely large number of lines in the `if` condition, it is very hard to figure out where the `else` statement starts. Please introduce new subroutines that split the wor as\n",
      "\n",
      "```\n",
      "if (.not. use_extrasnowlayers) then\n",
      "   call <approach-1>\n",
      "else\n",
      "   call <approach-2>\n",
      "end if\n",
      "```\n",
      "\n",
      "Repository:  E3SM\n",
      "Pull comment:  Please remove commentted out lines\n",
      "Repository:  E3SM\n",
      "Pull comment:  Please remove commentted out line\n",
      "Repository:  E3SM\n",
      "Pull comment:  Please remove commentted out line\n",
      "Repository:  E3SM\n",
      "Pull comment:  Please remove commentted out line\n",
      "Repository:  E3SM\n",
      "Pull comment:  Yes.  I will remove these lines for both cases in my next commit.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I'm not sure about this block.  I don't believe it (lines 660--663) is necessary, however, I'm not sure if removing it will maintain the model's BFB reproducibility.  Therefore I have \"removed\" these lines only for the new (use_extrasnowlayers) feature.\n",
      "Repository:  E3SM\n",
      "Pull comment:  Done\n",
      "Total comments for pull:  15\n",
      "-----------------------\n",
      "Repository:  E3SM\n",
      "Pull comment:  Why remove the `only:` clause?\n",
      "Repository:  E3SM\n",
      "Pull comment:  @xyuan , it looks like you're getting rid of passing this to the subroutine as an argument and instead storing it as a local data object upon initialization. Is that what you're doing? And if so, what is the  reason for doing this?\n",
      "Repository:  E3SM\n",
      "Pull comment:  Thanks for fixing this. This kills GNU too. I think we must not be testing MMF + GNU in any of our E3SM regressions.\n",
      "Repository:  E3SM\n",
      "Pull comment:  Why do this here? I though `#if defined(_OPENACC)` was done inside openacc_utils. This requires `#ifdef` everywhere we use it whereas having it inside `openacc_utils` looks a little cleaner.\n",
      "Repository:  E3SM\n",
      "Pull comment:  @xyuan , I think this is potentially a problem. `_OPENMP` is defined for any OpenMP-enabled compiler, but we only want this to be run if we're using OpenMP **4.5**. So I would recommend changing the define you pass to be `-D_OPENMP45` and to test for `#if defined(_OPENMP45)` instead of testing for `_OPENMP`.\n",
      "Repository:  E3SM\n",
      "Pull comment:  OpenACC directives are by default ignored if the compiler isn't OpenACC-capable and OpenACC isn't enabled. So there's no need to wrap these in `#ifdef`s I don't think.\n",
      "Repository:  E3SM\n",
      "Pull comment:  Why change `nyp1` to `ny+1`? Unless `nyp1` is causing a bug, I think we should keep it the same.\n",
      "Repository:  E3SM\n",
      "Pull comment:  This is a pretty sweeping change to the kernel. If there's a bug we're working around in OpenMP, then please create one version of the new manually collapsed kernel for OpenMP4.5, and leave the original kernel there with OpenACC.\n",
      "Repository:  E3SM\n",
      "Pull comment:  What is this addition here for?\n",
      "Repository:  E3SM\n",
      "Pull comment:  If this is a bug workaround, can you keep the OpenACC code as is and place an `#ifdef` for the workaround for IBM OpenMP4.5?\n",
      "Repository:  E3SM\n",
      "Pull comment:  This doesn't appear to be a case that needs atomics. You're allowed to read from i and i-1 in a loop so long as you aren't writing to the same variable. fu, fv, and fw are read-only, so it's OK to read from i and i-1.\n",
      "Repository:  E3SM\n",
      "Pull comment:  Again, I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Repository:  E3SM\n",
      "Pull comment:  I don't believe this needs atomics.\n",
      "Total comments for pull:  30\n",
      "-----------------------\n",
      "Repository:  E3SM\n",
      "Pull comment:  It's to remove compiler warnings about presence of `!$omp` pragmas in pureMPI compilation mode.\n",
      "Repository:  E3SM\n",
      "Pull comment:  `noomp` is compiler's default, turning on `omp` only for threaded builds.\n",
      "Repository:  E3SM\n",
      "Pull comment:  Having `ice_domain.F90` and `gw_common.F90` at `-O2` or `-O1` was producing non-BFB results in ERS tests of the `e3sm_integration` test suite. Compiling them with `-O0` made the tests BFB.\n",
      "Total comments for pull:  5\n",
      "-----------------------\n",
      "Repository:  lammps\n",
      "Pull comment:  I've submitted a change to the build_xxx_splines signatures to switch to passing a const reference to the vector.\n",
      "Repository:  lammps\n",
      "Pull comment:  in the current master error->message() accepts `const std::string &` as argument.\n",
      "Repository:  lammps\n",
      "Pull comment:  From the changes here, I don't see a benefit of using stl vectors over plain arrays, **if** you switch to using memory->create() and memory->destroy(). This will be more familiar to others reviewing and maintaining the code. So far we only switch to STL containers when there are significant benefits in syntax (`example: for (auto i : container) { ... }`) or readability (e.g. string compare with == instead of strcmp()).\n",
      "Repository:  lammps\n",
      "Pull comment:  With memory->create/destroy() this call to calloc/malloc is no longer needed.\n",
      "Repository:  lammps\n",
      "Pull comment:  This should be using the ValueTokenizer class.\n",
      "Repository:  lammps\n",
      "Pull comment:  When refactoring the code, you may also consider updating the source formatting. We prefer\n",
      "```\n",
      "if (condition) {\n",
      "    ...\n",
      "} else {\n",
      "   ...\n",
      "}\n",
      "```\n",
      "and equivalent for loops\n",
      "Repository:  lammps\n",
      "Pull comment:  > Checking for understanding here: Do you have a more general concern about the ability of vectors to handle reasonably large multidimensional arrays of doubles? The coder overhead is high to correctly manage the calloc/free code that was being used in this code before. A switch to vectors decreases that burden without having to change any of the code that accesses the actual array elements. Now that we have C++11 at our command, are there good reasons not to use vectors consistently for local data structures?\n",
      "\n",
      "the problem of the previous code is that it didn't use the memory class, which can allocate and delete multi-dimensional arrays in one go *and* nullify the pointer after deleting. That made it look clumsy. Also, the memory class has provisions for allocating memory with a preferred alignment and is integrated into the error reporting when malloc fails. When using C++ features you would also have to wrap everything into try/catch blocks to cleanly handle memory allocation issues and shut down parallel runs cleanly. My take on using containers is: we use them only when we can take advantage of their additional features, e.g. use iterators, or the size() member function (in case the size is unknown) or want to add/remove elements incrementally. What to use also depends on the specific use case. You can see that in the use of the tokenizer class (for speed) or the utils::split_words() function (for consistency and convenience).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total comments for pull:  9\n",
      "-----------------------\n",
      "Total comments for pull:  2\n",
      "-----------------------\n",
      "Repository:  lammps\n",
      "Pull comment:  I will look at those, thank you!\n",
      "Repository:  lammps\n",
      "Pull comment:  Bear with me on my next question. Does ilist always contain every owned local non-ghost id?\n",
      "Repository:  lammps\n",
      "Pull comment:  > Bear with me on my next question. Does ilist always contain every owned local non-ghost id?\n",
      "\n",
      "most of the time. not when atoms are explicitly excluded from the neighbor list. you can easily check for that with comparing `inum` to `atom->nlocal`\n",
      "Repository:  lammps\n",
      "Pull comment:  Excellent, first time really looking into LAMMPS neighbor lists so I am sorry for the basic questions.\n",
      "Repository:  lammps\n",
      "Pull comment:  also ilist does not contain ids (aka tags) but local indices, i.e. 0 to atom->nlocal. if you want the id you need `atom->tag[ilist[j] & NEIGHMASK]`. also you need to watch out for special_bond bits, hence the application of NEIGHMASK.\n",
      "Repository:  lammps\n",
      "Pull comment:  I assume, based on what you just said, there is no trivial atom->map() equivalent to go from tag to ilist index \"ii\".\n",
      "Repository:  lammps\n",
      "Pull comment:  I thought about this a while ago, but somehow got stuck on the neighbors-of-neighbors problem: moving one atom can change the force on an atom two neighbors over by chaning \\rho of the inbetween one. I think only atoms at 2* force cutoff can be safely ignored?\n",
      "Repository:  lammps\n",
      "Pull comment:  Style nitpick: replace with if/else\n",
      "Repository:  lammps\n",
      "Pull comment:  Not directly related to the new features, but while you're at it: replace all printfs with the new fmtlib calls?\n",
      "That would get rid of all those BIGINT_FORMAT concatenations.\n",
      "Repository:  lammps\n",
      "Pull comment:  imass calculation can be moved outside of the `if(folded)` branch, the value is identical for all beta iterations\n",
      "Repository:  lammps\n",
      "Pull comment:  command name in error message. or maybe ignore pending refactoring, see my overall comment?\n",
      "Repository:  lammps\n",
      "Pull comment:  Style nitpick: replace with if/else\n",
      "Repository:  lammps\n",
      "Pull comment:  fmtlib here as well\n",
      "Repository:  lammps\n",
      "Pull comment:  That is correct. The necessary but not sufficient condition for non-zero dFj/dRi is:\n",
      "    there exists atom k s.t. i and j are neighbors of k, where k is always a neighbor of itself.\n",
      "A fairly straightforward way to find all non-zero dFj/dRi is to first build a full neighbor list, then loop over atoms k, then do a double loop over neighbors of k, adding to a new list storing all ordered (i,j) pairs that satisfy the above criterion.  \n",
      "Repository:  lammps\n",
      "Pull comment:  Are the new calls \"%lli\"?\n",
      "Repository:  lammps\n",
      "Pull comment:  > I assume, based on what you just said, there is no trivial atom->map() equivalent to go from tag to ilist index \"ii\".\n",
      "\n",
      "why would you need that? is there anything that forces you in looping over tags in order? you can always get the tag from atom->tag[i] or atom->tag[j].\n",
      "Repository:  lammps\n",
      "Pull comment:  > I thought about this a while ago, but somehow got stuck on the neighbors-of-neighbors problem: moving one atom can change the force on an atom two neighbors over by chaning \\rho of the inbetween one. I think only atoms at 2* force cutoff can be safely ignored?\n",
      "\n",
      "unless you have long-range electrostatics or a model with dihedrals and a short pairwise cutoff (in which case you should get a warning to increase the communication cutoff).\n",
      "Repository:  lammps\n",
      "Pull comment:  and possibly also `utils::logmesg()` to have the text printed to both screen and logfile at no extra coding effort.\n",
      "Repository:  lammps\n",
      "Pull comment:  Just pushed a **draft** of my solution. I only search through first neighbors. I realize that I need to loop over their neighbors as well, but I figured it would be nice to have a draft to work off of.\n",
      "Repository:  lammps\n",
      "Pull comment:  New draft produces a neighbor/neighbor_of_neighbor list (extended neighbor list). This list is indexed by atom tag and yields a pointer to an array that contains the atom tags of the extended neighbors. Next step for me is to make this depend on the third order's group map member.\n",
      "Repository:  lammps\n",
      "Pull comment:  fmtlib looks a lot like python formatting, I like it already.\n",
      "Total comments for pull:  23\n",
      "-----------------------\n",
      "Total comments for pull:  2\n",
      "-----------------------\n",
      "Repository:  lammps\n",
      "Pull comment:  Will need to use a `Kokkos:: parallel_reduce`\n",
      "Repository:  lammps\n",
      "Pull comment:  Needs to be a `Kokkos::parallel_reduce` for energy and virial calcs\n",
      "Repository:  lammps\n",
      "Pull comment:  This got removed so is no longer required. \n",
      "Repository:  lammps\n",
      "Pull comment:  OK, great!\n",
      "Repository:  lammps\n",
      "Pull comment:  I don't know how to properly use that. I saw in various places code where Kokkos::parallel_reduce is used to reduce arrays to a single variable (e.g, a full sum), but in this case, the sum goes over all atoms on all processors that might be in the same body, and then they are all reduced to an array that is still over all bodies.\n",
      "I wrote it in terms of a Kokkos::parallel_for and then an MPI_Allreduce, but I do not know if that is the right way to do it. I can imagine the mandatory syncing to host before the MPI_Allreduce is not ideal.\n",
      "Repository:  lammps\n",
      "Pull comment:  I might need a hand with this or a pointer to another piece of code that does what needs to be done.\n",
      "Repository:  lammps\n",
      "Pull comment:  I actually only use this lambda once so we might as well remove it and paste the code verbatim. Will be more readable too.\n",
      "Repository:  lammps\n",
      "Pull comment:  Actually I looked at this more closely and the `sum` view needs to be atomic (more precisely a `Kokkos::ScatterView`, but I can add that later). Then you can pass the device `d_view.data()` pointers directly in the `MPI_Allreduce` call if the `cuda_aware_mpi` flag is enabled. Otherwise you will need to do the sync to host and then pass the host pointer to the Allreduce. \n",
      "Repository:  lammps\n",
      "Pull comment:  For example, see https://github.com/lammps/lammps/blob/master/src/KOKKOS/gridcomm_kokkos.cpp#L532\n",
      "Repository:  lammps\n",
      "Pull comment:  For example, see https://github.com/lammps/lammps/blob/master/src/KOKKOS/pair_coul_wolf_kokkos.cpp#L127 and https://github.com/lammps/lammps/blob/master/src/KOKKOS/pair_coul_wolf_kokkos.cpp#L296\n",
      "Repository:  lammps\n",
      "Pull comment:  For views in the `atomKK` class like `image` and `x`, don't directly call `sync/modify`, use this wrapper call instead: `atomKK->sync(IMAGE_FLAG|X_FLAG);` I.e.   because the wrapper potentially adds more operations.\n",
      "Repository:  lammps\n",
      "Pull comment:  These calls are redundant with line 772 (`atomKK->sync(execution_space, datamask_read);`) and should be removed.\n",
      "Repository:  lammps\n",
      "Pull comment:  This will crash on a GPU because the pointer lives on the host CPU. Must use a static function call or something that can be captured by the lambda, for example see https://github.com/lammps/lammps/blob/master/src/KOKKOS/fix_momentum_kokkos.cpp#L169.\n",
      "Repository:  lammps\n",
      "Pull comment:  Taken care of. Is there also a short-hand for syncing to the device? \n",
      "Repository:  lammps\n",
      "Pull comment:  Done, thanks.\n",
      "Repository:  lammps\n",
      "Pull comment:  Haha, that does not actually compile. Did you mean atomKK->sync(LMPHostType(), IMAGE_MASK | X_MASK)?\n",
      "Total comments for pull:  18\n",
      "-----------------------\n",
      "Repository:  lammps\n",
      "Pull comment:  please remove commented out code (unless it is specifically written for debugging)\n",
      "Repository:  lammps\n",
      "Pull comment:  nothing seems to be using anything from stdint.h here, so please remove. it should be avoided if at all possible in this kind of \"style header\" anyway.\n",
      "Repository:  lammps\n",
      "Pull comment:  no need to pull in pair.h, if you pull a header with a class derived from Pair.\n",
      "Repository:  lammps\n",
      "Pull comment:  please see comments for pair_CAC_sw.h\n",
      "Repository:  lammps\n",
      "Pull comment:  please see comments for pair_CAC_sw.h\n",
      "Repository:  lammps\n",
      "Pull comment:  please see comments for pair_CAC_sw.h\n",
      "Repository:  lammps\n",
      "Pull comment:  please see comments for pair_CAC_sw.h\n",
      "Repository:  lammps\n",
      "Pull comment:  nothing in this header uses `std::vector`. please remove. same for stdint.h.\n",
      "Repository:  lammps\n",
      "Pull comment:  this should not be included here, but include it in the implementation.\n",
      "Repository:  lammps\n",
      "Pull comment:  these need to be PIMPL-ified. that is, rather than accessing the data types directly as members of the PairCAC class, make them part of a standalone struct and then only store a pointer to that struct in the header. That allows using a forward declaration and thus bypasses the need to include \"asa_user.h\"\n",
      "Repository:  lammps\n",
      "Pull comment:  Please update and correct these header lines with error message summaries throughout all headers in your package and then also update the file `doc/src/Error_warnings.txt` and `doc/src/Error_messages.txt` accordingly.\n",
      "Repository:  lammps\n",
      "Pull comment:  as mentioned before, this style should be called `dump cac/nodal/positions`\n",
      "Repository:  lammps\n",
      "Pull comment:  this should be `<cmath>` and a few lines above the include should pull `<cstring>`. same for **all** files. also the system include files should be included first.\n",
      "Repository:  lammps\n",
      "Pull comment:  we bundle a basic BLAS/LAPACK package in `lib/linalg`, which is a subset of BLAS/LAPACK with only the required functions. So code can assume, that BLAS/LAPACK is always available. CMake will search for optimized BLAS first and then fall back to compiling the bundled library. for conventional build, the user has to read the docs,\n",
      "Repository:  lammps\n",
      "Pull comment:  please use C++ style include files for C library headers and try to utilize settings from lmptype.h as much as possible.\n",
      "Repository:  lammps\n",
      "Pull comment:  this should be `user-cac`. it will be capitalized anyway.\n",
      "Repository:  lammps\n",
      "Pull comment:  please do not add trailing whitepsace\n",
      "Repository:  lammps\n",
      "Pull comment:  Im a bit at loss as to how to refactor asa_user.h out of this header file given that the functions myvalue and mygrad depend on such a definition. Moving it all to another class that's forward declared then requires moving a large volume of data, specific to the objective of each calling class such as deciding on reneighbors vs. point to surface minimization; this is due to the requirement of defining myvalue and mygrad for the asa cg code to call. The interface between the function calls to the style implementation and the asa cg code is not simple since it relies on member function pointers with structure inputs.\n",
      "Repository:  lammps\n",
      "Pull comment:  I think I have an idea but it'll probably take a while to untangle the spider web. On the bright side it should also make adding new callers to the asa min algorithm easier in the future if it works as intended.\n",
      "Repository:  lammps\n",
      "Pull comment:  I'm guessing that this is the line being flagged by the doc build warnings but I'm confused as to the why.\n",
      "Repository:  lammps\n",
      "Pull comment:  This line starts with a full stop. That seems to me more the likely culprit, not the line above.\n",
      "Repository:  lammps\n",
      "Pull comment:  That was just my latest attempt to see where the chips fell. The period used to be right next to the Chen and now it should be one space apart on the same line from the Chen. I also used dos2unix to see if that was why.\n",
      "Repository:  lammps\n",
      "Pull comment:  Honestly not sure whether to just completely delete this possibility to use BLAS (since my problem dimensions are n=2, or n=3 and are likely not going to benefit anyway) or to leave it suggested in the docs in case someone happens to implement something that complicated and needs it. Is there a way to automate a check for whether BLAS is installed or not for the traditional build method?\n",
      "Repository:  lammps\n",
      "Pull comment:  There is no automated check in the conventional build. We have things set up that the default build uses the library from lib/linalg. You could put a folder `cac` into `lib` which works similar to others with a selection of pre-configured templates for `Makefile.lammps`. Or switch from BLAS to using `Eigen` like the USER-SMD package does (see `lib/smd`). Or just have replacement code inserted directly. Simple benchmarking should tell you whether the optimization of BLAS is really needed. For matrices with small ranks (< 10) a fully unrolled inline code (no pipeline stalling loops or ifs) can be much faster.\n",
      "Repository:  lammps\n",
      "Pull comment:  Yea I honestly can't foresee an application where the dimension will be higher than three for the uses in this package anyway. I'll probably just get rid of the BLAS related stuff, leaving the existing unrolled loops that have been used by default, and worst case scenario someone can just nab the code from the vanilla asa_cg algorithm again when they need that.\n",
      "Repository:  lammps\n",
      "Pull comment:  Needs to move `:all(b)`\n",
      "Repository:  lammps\n",
      "Pull comment:  Good catch.\n",
      "Total comments for pull:  29\n",
      "-----------------------\n",
      "Repository:  lammps\n",
      "Pull comment:  please do *NOT* modify standard LAMMPS makefiles in your branch. if you want a custom version, copy it to `src/MAKE/MINE` and edit as much as you like. Makefiles in `src/MAKE/MINE` take precedence over those with the same name in the other folders. (see the manual).\n",
      "Repository:  lammps\n",
      "Pull comment:  please also add the equivalent modification to `cmake/CMakeLists.txt` and test whether your package will also compile correctly with the CMake based build procedure.\n",
      "Repository:  lammps\n",
      "Pull comment:  please use C++ style include file names. you should not include `mpi.h` and include the include file for your fix first. please see `doc/include-file-conventions.md` for a summary of our conventions.\n",
      "Repository:  lammps\n",
      "Pull comment:  please find a better and more descriptive name for this fix. `hp` can stand for anything and does not give the user a hint what it may stand for (horse power? hewlett-packard? high-pitched?)\n",
      "Repository:  lammps\n",
      "Pull comment:  why is this needed? why not call `atom->find_custom()` directly?\n",
      "Repository:  lammps\n",
      "Pull comment:  *NEVER* commit code that belongs to an \"installed package\". in fact, instead the names of the files in the package should be added to the `.gitignore` file in the `src` folder, so that this won't happen by accident.\n",
      "Repository:  lammps\n",
      "Pull comment:  i think these functions names would be ok without `peratom`. the `gather` implies that it is about a per-atom property.\n",
      "you should also add a test whether the fix has the `peratom_flag` and then `size_peratom_cols` will give you the information whether it is a per-atom vector or array. there is also the peratom_freq flag, that provides information when the data is available and thus the function should fail if not.\n",
      "Repository:  lammps\n",
      "Pull comment:  I based my fix on fix_langevin, which also includes `mpi.h`, though I have now include `fix_pafi.h` first. \n",
      "Repository:  lammps\n",
      "Pull comment:  I now have removed all changes from `atom.cpp` to `library.cpp`\n",
      "Repository:  lammps\n",
      "Pull comment:  I have now replaced all calls to lmp->atom->extract in `library.cpp` with `lammps_extract_atom` and added lines in this function to check for `property/atom` vectors. This felt like the lightest fix...\n",
      "Repository:  lammps\n",
      "Pull comment:  there is no need to include \"compute.h\" here. just use a forward declaration like with RanMars.\n",
      "Repository:  lammps\n",
      "Pull comment:  this is still not using C++ style include files. I am not very fussy about `mpi.h`, but for using consistently C++ style include files instead of the C style includes, there is a known benefit. in several cases there can otherwise be problem with inconsistent includes if a mix of some of these are included and some header files must include c-library headers and consequently include the C++ version.\n",
      "Repository:  lammps\n",
      "Pull comment:  are these used anywhere?\n",
      "Repository:  lammps\n",
      "Pull comment:  **all** class members that are pointers should be initialized to NULL here. This is to make certain, that when using `clear` or unfix/fix to delete and create classes, all pointers are always NULL when not allocated and thus can be safely deleted/destroyed in the destructor.\n",
      "Repository:  lammps\n",
      "Pull comment:  error messages should refer to `fix pafi` and not `fix hp`\n",
      "Repository:  lammps\n",
      "Pull comment:  I think you need to set `time_integrate = 1;` here. You do updates of velocities and positions, right?\n",
      "Repository:  lammps\n",
      "Pull comment:  Have you checked, if this is applicable? \n",
      "Repository:  lammps\n",
      "Pull comment:  you should check here, if the \"PathCompute\" compute is still valid. it may have been deleted since creating the fix.\n",
      "Repository:  lammps\n",
      "Pull comment:  not a big deal right now, but if you use `utils::strmatch(update->minimize_style,\"^fire\")` it will also match suffix versions, in case one uses accelerated styles.\n",
      "Repository:  lammps\n",
      "Pull comment:  for adding new functions to library.cpp, you may want to check out the \"progguide\" branch, where I am currently reworking the library interface files to have the documentation of the library interface automatically generated from the comments in the source code. you can watch the progress currently at https://docs.lammps/org (scroll to the bottom part of the navigation sidebar).\n",
      "Repository:  lammps\n",
      "Pull comment:  I have pushed a new `library.cpp` to the \"scattergather\" branch of my fork, where I have put all the fix / compute / property/atom checks into the set of functions\n",
      "```\n",
      "void lammps_gather(void *, char *, int, int, void *);\n",
      "void lammps_gather_concat(void *, char *, int, int, void *);\n",
      "void lammps_gather_subset(void *, char *, int, int, int, int *, void *);\n",
      "void lammps_scatter(void *, char *, int, int, void *);\n",
      "void lammps_scatter_subset(void *, char *, int, int, int, int *, void *);\n",
      "```\n",
      "where name = \"x\" , \"f\" or other atom properties\n",
      "        \"d_name\" or \"i_name\" for fix property/atom quantities\n",
      "        \"f_fix\", \"c_compute\" for fixes / computes\n",
      "        will return error if fix/compute doesn't isn't atom-based\n",
      "\n",
      "These could wrap gather_atoms scatter_atoms etc but also allow more general use. I will look at the documentation format asap\n",
      "Repository:  lammps\n",
      "Pull comment:  If you think this is a better idea than the `gather_fix_* functions` then I will merge the branches\n",
      "Repository:  lammps\n",
      "Pull comment:  I currently cannot comment on that, since I have not reached the scatter/gather library functions in my refactoring in #1970 and thus do not yet have a sense of what is good there or not.\n",
      "Repository:  lammps\n",
      "Pull comment:  It is not due to the physics behind the constraint. Thanks!\n",
      "Repository:  lammps\n",
      "Pull comment:  This was there but lower down. Corrected\n",
      "Repository:  lammps\n",
      "Pull comment:  no\n",
      "Repository:  lammps\n",
      "Pull comment:  I have changed everything to C++ style\n",
      "Repository:  lammps\n",
      "Pull comment:  For usage of the fix either of the branches are OK, so whatever you decide.\n",
      "Total comments for pull:  30\n",
      "-----------------------\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Add `#end def` closure.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Add `#end def` closure.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Add `#end def` closure.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Add `#end def` closure.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Add `#end for` and `#end if` closures to the below.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Add `#end` closures for `def`, `for`, and `if`.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Add `#end` closures throughout.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Restore `#end try` closure.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Restore encapsulating `try` statement.  This will prevent the analyzer from ever halting Nexus execution.  Partial information available after analysis is a downstream problem.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Add `#end` closures.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Restore warning within the `except` block.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Avoid `eval` if at all possible (code injection vulnerability).  Does `float()` not work here?\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Restore fermi energy processing.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Restore bands processing.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Restore alternate xml data path.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Add `#end` closures.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  `md_statistics` and `md_plots` appear to be repeated with some variations in a few files.  Is there a good way to consolidate and avoid duplication?\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Restore alias for `exit` function.\n",
      "Total comments for pull:  20\n",
      "-----------------------\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Please revise function names in this file following our function naming rules. For example here, calculateMadelungConstant(). This function also doesn't modify any class member, so please mark this function const.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  if we define a member function addZeroContribution() to both `T& function`, there is no need of the `zero` switch.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  When adding new code to the source, how do you determine what needs separate namspace(s) and what does not?  Most of the code simply falls within the qmcplusplus namespace (which really should be `qmcpack` since the name of the code changed from QMC++ long ago...). \n",
      "Repository:  QMCPACK\n",
      "Pull comment:  Not time efficient.  I'm waiting for the automatic part of #794 to be implemented.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  The naming already follows the coding conventions, as in the manual:\n",
      "\n",
      "```\n",
      "27.3.5 (Member) function names\n",
      "Function names should start with a lowercase character and have a capital letter for each new word.\n",
      "```\n",
      "\n",
      "Adding `calculate` to every function that calculates something is IMO trite and lexically wasteful.\n",
      "\n",
      "Will add const.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  The term should not need to know how it is being summed.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  There are two topics.\n",
      "1. header file \"using namespace\" is a bad practice. cpp file is fine. \"using namespace\" propagates in header files but not in cpp files.\n",
      "2. There is no strict rule whether extra namespace should be used or not. The original EwaldRef.h defines a few datatypes directly in the qmcplusplus name space. They can propagate and can be misused or cause potential name collisions. So I put most of the content of EwaldRef.h in ewaldref namespace within qmcplusplus namespace as a protection. Here the added code is quite standalone and closely related to EwaldRef then putting it in the same namespace as EwaldRef seems appropriate.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  I'm fine with this, though the new code is not going to be used as reference.  If it has a future, it will be in the main application, otherwise it will be deleted.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  `madelungConstant()` is not clear whether it to access a stored value in the class or run a calculation. When I name functions, evaluate/calculateXXX() means it can be costly. getXXX() means cheap. Probably in your case, it doesn't matter much.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  If you intend to implement a class derived from OperatorBase, then don't put the class in ewaldref namespace. Instead, you can either put `ewaldref::` for using any piece from EwaldRef or \"using ewaldref::ABC\" in the class to bring in ABC to the new class.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  ignore my last comment. zero and function need to be controlled independently.\n",
      "Repository:  QMCPACK\n",
      "Pull comment:  The automatic part has been implemented; see https://github.com/QMCPACK/qmcpack/wiki/Source-formatting\n",
      "\n",
      "This was a decision made as a project to not have a robot do formatting without supervision, but to provide various methods for different workflow preferences that everyone can choose for themselves, from assisted manual to full automatic.\n",
      "Total comments for pull:  14\n",
      "-----------------------\n",
      "Total comments for pull:  2\n",
      "-----------------------\n",
      "Repository:  enzo-project\n",
      "Pull comment:  As noted in another comment, this could include subgrids by checking ParentGrid (to see if it is NULL).\n",
      "Repository:  enzo-project\n",
      "Pull comment:  Thanks for catching this. After a star particle is formed, the \"accretion sphere\" is replaced with a uniform density. The metal field was only reduced by that factor (0.57), but the more correct thing to do this is to keep the same metallicity as we change the density.\n",
      "\n",
      "I'll add a note in the PR description.\n",
      "Repository:  enzo-project\n",
      "Pull comment:  I like this approach a lot better. I've made your suggested change.\n",
      "Repository:  enzo-project\n",
      "Pull comment:  I seem to recall that `debug1` results in a lot of output. Would `debug` be better?\n",
      "Repository:  enzo-project\n",
      "Pull comment:  Won't this set the `SNColourNum` field to the same value as the `MetalNum` field? Is that the right thing to do?\n",
      "Repository:  enzo-project\n",
      "Pull comment:  Did this block of code move in a critical way?\n",
      "Total comments for pull:  8\n",
      "-----------------------\n",
      "Repository:  enzo-project\n",
      "Pull comment:  nitpick:  \"this will\" -> \"this will work\"\n",
      "Repository:  enzo-project\n",
      "Pull comment:  Maybe specify here where to find it for the anisotropic bits\n",
      "Repository:  enzo-project\n",
      "Pull comment:  Maybe also specify here which methods have this currently implemented (as is done in `index.rst`)\n",
      "Repository:  enzo-project\n",
      "Pull comment:  Is the `FIXME` something still need doing?\n",
      "Repository:  enzo-project\n",
      "Pull comment:  maybe a comment here as to what `iCR` is and what the hard-coded values mean?\n",
      "Repository:  enzo-project\n",
      "Pull comment:  Does the `FIXME` still need doing?\n",
      "Repository:  enzo-project\n",
      "Pull comment:  `FIXME` \n",
      "Repository:  enzo-project\n",
      "Pull comment:  Minor point, but any reason why this was switched to a  hard-coded pi instead of the `pi` parameter?\n",
      "Repository:  enzo-project\n",
      "Pull comment:  Similar question with this hard-coded pi instead of the gloabal `pi`  (see comment in `Grid_CRShockTubeInitializeGrid.C`)\n",
      "Repository:  enzo-project\n",
      "Pull comment:  I've always wondered how much of a difference this actually matters in practice, but could you do something like `inv_sqrt_rho = 1/sqrt(rho)` be saved as its own variable and then have `* sqrt_rho` here? And similar things with `dx` as `inv_dx`? I've certainly not always optimized my own pieces of code this way but I've always been under the impression its important to do (but maybe someone else knows whether or not this makes a big difference in the grand scheme of things)\n",
      "Repository:  enzo-project\n",
      "Pull comment:  missing a `delete [] dx;` here\n",
      "Repository:  enzo-project\n",
      "Pull comment:  Should this Makefile get pulled in? (and is it really for Ubuntu 8.04?)  \n",
      "Repository:  enzo-project\n",
      "Pull comment:  same comment about making a  `inv_sqrt_rho = 1/sqrt(rho)` variable here as in `Grid_ComputeCRStreaming.C`\n",
      "Repository:  enzo-project\n",
      "Pull comment:  Hmm... I don't think I made the set of FIXME comments / or used cosmology.  The CosmologySimulationUniformCR parameter looks fine to me. It might be just a leftover comment? I'm happy to delete it, but I was hesitant to in case there's something I'm missing. \n",
      "Repository:  enzo-project\n",
      "Pull comment:  I'm not sure why this was switched back... I agree it should be pi. \n",
      "Repository:  enzo-project\n",
      "Pull comment:  I added a new inv_sqrt_rho variable but not inv_dx since I don't think it will change the number of computations (could be wrong though). \n",
      "Repository:  enzo-project\n",
      "Pull comment:  Ooops! That wasn't supposed to be part of the PR. \n",
      "Total comments for pull:  19\n",
      "-----------------------\n",
      "Repository:  enzo-project\n",
      "Pull comment:  Debian-based XeonPhi machine\n",
      "Repository:  enzo-project\n",
      "Pull comment:  Debian-based XeonPhi machine\n",
      "Repository:  enzo-project\n",
      "Pull comment:  A more general description would be good. \"Debian-based XeonPhi machine\"\n",
      "Total comments for pull:  5\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import getpass\n",
    "\n",
    "project_dict = {\n",
    "    'Moose': 'idaholab/moose',\n",
    "    'Spack': 'spack/spack',\n",
    "    'yt': 'yt-project/yt',\n",
    "    'petsc': 'petsc/petsc',\n",
    "    'E3SM': 'E3SM-Project/E3SM',\n",
    "    'lammps': 'lammps/lammps',\n",
    "    'gromacs': 'gromacs/gromacs',\n",
    "    'OSGConnect': 'OSGConnect/TOREVIEW-tutorial-namd',\n",
    "    'QMCPACK': 'QMCPACK/qmcpack',\n",
    "    'Nek5000': 'Nek5000/Nek5000',\n",
    "    'nwchemgit': 'nwchemgit/nwchem',\n",
    "    # 'ECP-astro': '???',\n",
    "    'lanl': 'lanl/LATTE',\n",
    "    'CRL': 'gridaphobe/CRL',\n",
    "    'enzo-project': 'enzo-project/enzo-dev'\n",
    "}\n",
    "\n",
    "\n",
    "def retrieve_pull_info(uname, pwd):\n",
    "    for project in project_dict:\n",
    "        pull_url = 'https://api.github.com/repos/%s/pulls' % project_dict[project]\n",
    "        pull_response = requests.get(pull_url, auth=(uname, pwd))\n",
    "        pull_json_response = pull_response.json()\n",
    "\n",
    "        item_count = 0\n",
    "        page_no = 1\n",
    "        max_page = False\n",
    "        # Parse through JSON response of API for comments list\n",
    "        while not max_page:\n",
    "            for item in pull_json_response:\n",
    "                # Skip if no comments url\n",
    "                if item['review_comments_url'] is None:\n",
    "                    item_count += 1\n",
    "                    continue\n",
    "\n",
    "                # If comments url exists jump to it\n",
    "                comments_url = item['review_comments_url']\n",
    "                comments_response = requests.get(comments_url, auth=(uname, pwd))\n",
    "                comments_json_response = comments_response.json()\n",
    "                comment_count = 0\n",
    "\n",
    "                if not comments_json_response:\n",
    "                    continue\n",
    "                else:\n",
    "                    for comment in comments_json_response:\n",
    "                        if comment_count > 1:\n",
    "                            print('Repository: ', project)\n",
    "                            print('Pull comment: ', comment['body'])\n",
    "                        comment_count += 1\n",
    "\n",
    "                    if comment_count > 1:\n",
    "                        str(comment_count)\n",
    "                        print('Total comments for pull: ', comment_count)\n",
    "                        print('-----------------------')\n",
    "\n",
    "                item_count += 1\n",
    "\n",
    "            if item_count == 30:\n",
    "                page_no += 1\n",
    "                pull_url += '?&page=%s' % str(page_no)\n",
    "                pull_response = requests.get(pull_url, auth=(uname, pwd))\n",
    "                pull_json_response = pull_response.json()\n",
    "                item_count = 0\n",
    "\n",
    "            else:\n",
    "                max_page = True\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    username = input('GitHub Authentication Username: ')\n",
    "    # password = getpass.getpass(prompt='Password: ')\n",
    "    password = getpass.getpass(prompt='Password: ')\n",
    "    retrieve_pull_info(username, password)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "development_patterns_IDEAS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
